{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "bridge_trajs = np.load(\"assets/bridge_v2_10_trajs.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_autoeval_robot_benchmark.utils.ecot_primitives import ecot_primitive_movements, inverse_ecot_primitive_movements\n",
    "from vlm_autoeval_robot_benchmark.models.vlm import VLM, create_vlm_request, parse_vlm_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trajectory_video(descriptors, delay=0.2, wait_for_key=False):\n",
    "    \"\"\"Show observations as a video with either delay between frames or keystroke.\n",
    "    \n",
    "    Args:\n",
    "        descriptors: Dictionary containing trajectory information\n",
    "        delay: Time delay between frames if wait_for_key is False\n",
    "        wait_for_key: If True, wait for any key press between frames\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, obs in enumerate(descriptors[\"obs\"]):\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(obs)\n",
    "        title = f\"Frame {i} - {descriptors['task_language_instruction']}\"\n",
    "        for k,v in descriptors['moves'][i].items():\n",
    "            print(f\"{k}\")\n",
    "            print(f\"\\ntop {v[0]}, {type(v[0])}, {v[1]}, {type(v[1])}\")\n",
    "            text_actions = f\"{v[0]} - {v[1].tolist()}\" \n",
    "            print(f\"\\nbottom {descriptors['gt_actions'][i]}, {type(descriptors['gt_actions'][i])}\")\n",
    "            title += f\"\\n------------------------\\n{k} - {text_actions}\"\n",
    "        gt_actions = descriptors['gt_actions'][i].tolist()\n",
    "        title += f\"\\n------------------------\\nGT actions: {gt_actions}\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        display(plt.gcf())\n",
    "        \n",
    "        if wait_for_key:\n",
    "            input(\"Press Enter to continue...\")  # Wait for any key\n",
    "        else:\n",
    "            time.sleep(delay)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_to_episode(traj):\n",
    "    steps = []\n",
    "    for i in range(len(traj[\"observations\"])):\n",
    "        step = {}\n",
    "        step[\"observation\"] = traj[\"observations\"][i]\n",
    "        step[\"action\"] = traj[\"actions\"][i]\n",
    "        steps.append(step)\n",
    "    return dict(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.00\n",
    "WINDOW = 2\n",
    "GRIPPER_INDEX = 6\n",
    "GRIPPER_OPEN_THRESHOLD = 0.95\n",
    "\n",
    "def get_gripper_position(gripper_state: float) -> str:\n",
    "    return \"OPEN\" if gripper_state > GRIPPER_OPEN_THRESHOLD else \"CLOSED\"\n",
    "\n",
    "def get_descriptors(traj):\n",
    "    move_primitives = ecot_primitive_movements.get_move_primitives_episode(repackage_to_episode(traj), threshold=THRESHOLD, window=WINDOW)\n",
    "    move_primitives = [dict(ecot=move) for move in move_primitives]\n",
    "    obs_list = [t[\"images0\"] for t in traj[\"observations\"]]  # list of obs\n",
    "    gt_actions = traj[\"actions\"]  # list of ground truth actions\n",
    "    gripper_states = [get_gripper_position(obs[\"state\"][GRIPPER_INDEX]) for obs in traj[\"observations\"]]\n",
    "    task_language_instruction = traj[\"language\"][0] if \"language\" in traj else None\n",
    "    return dict(moves=move_primitives, obs=obs_list, gt_actions=gt_actions, task_language_instruction=task_language_instruction, gripper_states=gripper_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the tasks that are phrased in the PAST tense are messing up the models \n",
    "\n",
    "\"\"\"\n",
    "0 abriu a gaveta\n",
    "1 close the drawer\n",
    "2 removed the green thing from the drawer and placed it on the left side of the table.\n",
    "3 take the red object out of the pot and put it on the left burner\n",
    "4 take the blue stuffed animal and leaves it inside the drawer.\n",
    "5 open the drawer\n",
    "6 close the drawer\n",
    "7 Open the drawer\n",
    "8 close the drawer\n",
    "9 removed the blue object from the drawer and put it on the lower left side of the table\n",
    "\"\"\"\n",
    "\n",
    "task_edits = {\n",
    "    0: \"open the drawer\",\n",
    "    2: \"remove the green thing from the drawer and place it on the left side of the table\",\n",
    "    4: \"take the blue stuffed animal and leave it inside the drawer\",\n",
    "    9: \"remove the blue object from the drawer and put it on the lower left side of the table\"\n",
    "}\n",
    "for k,v in task_edits.items():\n",
    "    bridge_trajs[k][\"language\"][0] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_idx = 2  # Change this to visualize different trajectories\n",
    "episode_descriptors = get_descriptors(bridge_trajs[traj_idx])\n",
    "# show_trajectory_video(episode_descriptors, delay=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, traj in enumerate(bridge_trajs):\n",
    "    descriptors = get_descriptors(traj)\n",
    "    print(i, descriptors['task_language_instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primitive_moves_tester import run_test, print_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = bridge_trajs[traj_idx]\n",
    "episode_descriptors = get_descriptors(bridge_trajs[traj_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def numpy_array_to_png_bytes(arr: np.ndarray) -> bytes:\n",
    "    \"\"\"\n",
    "    Convert a NumPy array to PNG file bytes, as if it was saved as a PNG and then read with fp.read()\n",
    "    \n",
    "    Args:\n",
    "        arr: NumPy array with shape (height, width, 3) and dtype uint8\n",
    "        \n",
    "    Returns:\n",
    "        PNG file bytes\n",
    "    \"\"\"\n",
    "    # Ensure the array is the right shape and type\n",
    "    if len(arr.shape) != 3 or arr.shape[2] != 3:\n",
    "        raise ValueError(f\"Expected array with shape (height, width, 3), got {arr.shape}\")\n",
    "    \n",
    "    if arr.dtype != np.uint8:\n",
    "        arr = arr.astype(np.uint8)\n",
    "    \n",
    "    # Convert the NumPy array to a PIL Image\n",
    "    img = Image.fromarray(arr)\n",
    "    \n",
    "    # Create a BytesIO object to store the image bytes\n",
    "    buffer = io.BytesIO()\n",
    "    \n",
    "    # Save the image to the BytesIO object as PNG\n",
    "    img.save(buffer, format=\"PNG\")\n",
    "    \n",
    "    # Get the bytes from the BytesIO object\n",
    "    png_bytes = buffer.getvalue()\n",
    "    \n",
    "    return png_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_episode_descriptors = dict()\n",
    "sub_step_interval = 5  \n",
    "\n",
    "for k,v in episode_descriptors.items():\n",
    "    if isinstance(v, list):\n",
    "        sub_episode_descriptors[k] = v[::sub_step_interval]\n",
    "    else:\n",
    "        sub_episode_descriptors[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"gpt-4o\"\n",
    "# model = \"gemini/gemini-2.0-flash\"\n",
    "# model = \"gemini/gemini-2.0-pro-exp\"\n",
    "model = \"gemini/gemini-2.5-pro-preview-03-25\"\n",
    "\n",
    "env_desc = \"You are looking at a wooden desk with a black robot arm. The desk has a drawer with a handle and some objects on it.\"\n",
    "task_desc = sub_episode_descriptors[\"task_language_instruction\"]\n",
    "img_bytes_list = [numpy_array_to_png_bytes(img) for img in sub_episode_descriptors[\"obs\"]]\n",
    "gripper_descriptors_list = sub_episode_descriptors[\"gripper_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_autoeval_robot_benchmark.models.vlm import parse_vlm_responses\n",
    "import litellm\n",
    "litellm._turn_on_debug()\n",
    "\n",
    "HISTORY_PREFIX = \"This shows the history of a robotics episode.\"\n",
    "HISTORY_SUFFIX = \"Consider this history to answer the question below. Describe the history in detail before answering the question.\"\n",
    "\n",
    "async def run_episode(model, env_desc, task_desc, img_bytes_list, gripper_descriptors):\n",
    "    vlm = VLM()\n",
    "    reqs = []\n",
    "    input_history = []\n",
    "    for i in range(len(img_bytes_list)):\n",
    "        if input_history:\n",
    "            current_history = dict(prefix=HISTORY_PREFIX, vlm_inputs=[input_history[-min(2, len(input_history))]], suffix=HISTORY_SUFFIX)\n",
    "        else:\n",
    "            current_history = None\n",
    "        reqs.append(create_vlm_request(model, img_bytes_list[i], env_desc, task_desc, gripper_position=gripper_descriptors[i], history_dict=current_history))\n",
    "        input_history.append((\"Historical image\", [img_bytes_list[i]]))\n",
    "    responses = await vlm.generate_parallel(reqs)\n",
    "    results = parse_vlm_responses(responses)\n",
    "    return results, responses, reqs\n",
    "\n",
    "# Run all tests in parallel\n",
    "results, responses, reqs = asyncio.run(run_episode(\n",
    "    model,\n",
    "    env_desc,\n",
    "    task_desc,\n",
    "    img_bytes_list,\n",
    "    gripper_descriptors_list\n",
    "))\n",
    "print_test_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = [r for r in results if 'answer' not in r]\n",
    "print(f\"num bad: {len(bad)}\")\n",
    "if len(bad) > 0:\n",
    "    print(bad[0]['error'])\n",
    "    print(bad[0]['raw_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for t, res in enumerate(results):\n",
    "    description = res['description']\n",
    "    sub_episode_descriptors['moves'][t]['vlm - desc'] = (\"\\n\".join([description[x:x+100] for x in range(0, len(description), 100)]), np.array([]))\n",
    "\n",
    "    action_texts = []\n",
    "    for k in ['x', 'y', 'z', 'tilt', 'roll', 'rotation', 'gripper']:\n",
    "        action_texts.append(f\"{res['answer'][k][0]}\" + (f\"({res['answer'][k][1]})\" if res['answer'][k][1] != 0.0 else \"\"))\n",
    "    \n",
    "    calculated_actions = inverse_ecot_primitive_movements.text_to_move_vector(res['answer'])\n",
    "    formatted_action_text = \" , \".join([a for a in action_texts if a != 'None'])\n",
    "    sub_episode_descriptors['moves'][t]['vlm'] = (formatted_action_text, calculated_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]['answer']['gripper']\n",
    "# for res in results[7:]:\n",
    "#     print(inverse_ecot_primitive_movements.text_to_move_vector(res['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trajectory_video(sub_episode_descriptors, delay=5, wait_for_key=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lapa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
