{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "bridge_trajs = np.load(\"assets/bridge_v2_10_trajs.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_autoeval_robot_benchmark.utils.ecot_primitives import ecot_primitive_movements, inverse_ecot_primitive_movements\n",
    "from vlm_autoeval_robot_benchmark.models.vlm import VLM, create_vlm_request, parse_vlm_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trajectory_video(descriptors, delay=0.2, wait_for_key: bool=False, action_threshold: float = .00001):\n",
    "    \"\"\"Show observations as a video with either delay between frames or keystroke.\n",
    "    \n",
    "    Args:\n",
    "        descriptors: Dictionary containing trajectory information\n",
    "        delay: Time delay between frames if wait_for_key is False\n",
    "        wait_for_key: If True, wait for any key press between frames\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, obs in enumerate(descriptors[\"obs\"]):\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(obs)\n",
    "        title = f\"Frame {i} - {descriptors['task_language_instruction']}\"\n",
    "        for k,v in descriptors['moves'][i].items():\n",
    "            text_actions = f\"{v[0]} - {v[1].tolist()}\" \n",
    "            title += f\"\\n------------------------\\n{k} - {text_actions}\"\n",
    "\n",
    "        gt_actions = [x if abs(x) > action_threshold else 0.0 for x in descriptors['gt_actions'][i].tolist()]\n",
    "        gt_actions_text = [inverse_ecot_primitive_movements.unnormalize_bounded_action(k, x) for k, x in zip(ecot_primitive_movements.DIRECTION_NAMES.keys(), gt_actions)]\n",
    "        gt_actions_text = [(round(magnitude, 4), direction) for magnitude, direction in gt_actions_text if direction != \"None\"]\n",
    "        title += f\"\\n------------------------\\nGT actions ECOT: {gt_actions_text}\\nREAL GT: {gt_actions}\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        display(plt.gcf())\n",
    "        \n",
    "        if wait_for_key:\n",
    "            input(\"Press Enter to continue...\")  # Wait for any key\n",
    "        else:\n",
    "            time.sleep(delay)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_to_episode(traj):\n",
    "    steps = []\n",
    "    for i in range(len(traj[\"observations\"])):\n",
    "        step = {}\n",
    "        step[\"observation\"] = traj[\"observations\"][i]\n",
    "        step[\"action\"] = traj[\"actions\"][i]\n",
    "        steps.append(step)\n",
    "    return dict(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_autoeval_robot_benchmark.utils.robot_utils import get_gripper_position, GRIPPER_INDEX\n",
    "\n",
    "THRESHOLD = 0.00\n",
    "WINDOW = 2\n",
    "\n",
    "\n",
    "def get_descriptors(traj):\n",
    "    move_primitives = ecot_primitive_movements.get_move_primitives_episode(repackage_to_episode(traj), threshold=THRESHOLD, window=WINDOW)\n",
    "    move_primitives = [dict(ecot=move) for move in move_primitives]\n",
    "    obs_list = [t[\"images0\"] for t in traj[\"observations\"]]  # list of obs\n",
    "    gt_actions = traj[\"actions\"]  # list of ground truth actions\n",
    "    gripper_states = [get_gripper_position(obs[\"state\"][GRIPPER_INDEX]) for obs in traj[\"observations\"]]\n",
    "    task_language_instruction = traj[\"language\"][0] if \"language\" in traj else None\n",
    "    return dict(moves=move_primitives, obs=obs_list, gt_actions=gt_actions, task_language_instruction=task_language_instruction, gripper_states=gripper_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the tasks that are phrased in the PAST tense are messing up the models \n",
    "\n",
    "\"\"\"\n",
    "0 abriu a gaveta\n",
    "1 close the drawer\n",
    "2 removed the green thing from the drawer and placed it on the left side of the table.\n",
    "3 take the red object out of the pot and put it on the left burner\n",
    "4 take the blue stuffed animal and leaves it inside the drawer.\n",
    "5 open the drawer\n",
    "6 close the drawer\n",
    "7 Open the drawer\n",
    "8 close the drawer\n",
    "9 removed the blue object from the drawer and put it on the lower left side of the table\n",
    "\"\"\"\n",
    "\n",
    "task_edits = {\n",
    "    0: \"open the drawer\",\n",
    "    2: \"remove the green thing from the drawer and place it on the left side of the table\",\n",
    "    4: \"take the blue stuffed animal and leave it inside the drawer\",\n",
    "    9: \"remove the blue object from the drawer and put it on the lower left side of the table\"\n",
    "}\n",
    "for k,v in task_edits.items():\n",
    "    bridge_trajs[k][\"language\"][0] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_idx = 4  # Change this to visualize different trajectories\n",
    "episode_descriptors = get_descriptors(bridge_trajs[traj_idx])\n",
    "# show_trajectory_video(episode_descriptors, delay=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, traj in enumerate(bridge_trajs):\n",
    "    descriptors = get_descriptors(traj)\n",
    "    print(i, descriptors['task_language_instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primitive_moves_tester import run_test, print_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = bridge_trajs[traj_idx]\n",
    "episode_descriptors = get_descriptors(bridge_trajs[traj_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_autoeval_robot_benchmark.utils.image_utils import numpy_array_to_png_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_episode_descriptors = dict()\n",
    "sub_step_interval = 5  \n",
    "\n",
    "for k,v in episode_descriptors.items():\n",
    "    if isinstance(v, list):\n",
    "        sub_episode_descriptors[k] = v[::sub_step_interval]\n",
    "    else:\n",
    "        sub_episode_descriptors[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"gpt-4o\"\n",
    "# model = \"gemini/gemini-2.0-flash\"\n",
    "# model = \"gemini/gemini-2.0-pro-exp\"\n",
    "model = \"gemini/gemini-2.5-pro-preview-03-25\"\n",
    "\n",
    "env_desc = \"You are looking at a wooden desk with a black robot arm. The desk has a drawer with a handle and some objects on it.\"\n",
    "task_desc = sub_episode_descriptors[\"task_language_instruction\"]\n",
    "img_bytes_list = [numpy_array_to_png_bytes(img) for img in sub_episode_descriptors[\"obs\"]]\n",
    "gripper_descriptors_list = sub_episode_descriptors[\"gripper_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_autoeval_robot_benchmark.models.eval import run_episode\n",
    "\n",
    "# Run all tests in parallel\n",
    "reqs, responses, results = asyncio.run(run_episode(\n",
    "    model,\n",
    "    env_desc,\n",
    "    task_desc,\n",
    "    img_bytes_list,\n",
    "    gripper_descriptors_list\n",
    "))\n",
    "print_test_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = [r for r in results if 'answer' not in r]\n",
    "print(f\"num bad: {len(bad)}\")\n",
    "if len(bad) > 0:\n",
    "    print(bad[0]['error'])\n",
    "    print(bad[0]['raw_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for t, res in enumerate(results):\n",
    "    description = res['description']\n",
    "    sub_episode_descriptors['moves'][t]['vlm - desc'] = (\"\\n\".join([description[x:x+100] for x in range(0, len(description), 100)]), np.array([]))\n",
    "\n",
    "    action_texts = []\n",
    "    for k in ['x', 'y', 'z', 'tilt', 'roll', 'rotation', 'gripper']:\n",
    "        action_texts.append(f\"{res['answer'][k][0]}\" + (f\"({res['answer'][k][1]})\" if res['answer'][k][1] != 0.0 else \"\"))\n",
    "    \n",
    "    calculated_actions = inverse_ecot_primitive_movements.text_to_move_vector(res['answer'])\n",
    "    formatted_action_text = \" , \".join([a for a in action_texts if a != 'None'])\n",
    "    sub_episode_descriptors['moves'][t]['vlm'] = (formatted_action_text, calculated_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trajectory_video(sub_episode_descriptors, delay=5, wait_for_key=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lapa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
