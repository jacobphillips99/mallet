{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "bridge_trajs = np.load(\"assets/bridge_v2_10_trajs.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_autoeval_robot_benchmark.utils.ecot_primitives import ecot_primitive_movements, inverse_ecot_primitive_movements\n",
    "from vlm_autoeval_robot_benchmark.models.vlm import VLM, create_vlm_request, parse_vlm_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_trajectory_video(descriptors, delay=0.2, wait_for_key=False):\n",
    "    \"\"\"Show observations as a video with either delay between frames or keystroke.\n",
    "    \n",
    "    Args:\n",
    "        descriptors: Dictionary containing trajectory information\n",
    "        delay: Time delay between frames if wait_for_key is False\n",
    "        wait_for_key: If True, wait for any key press between frames\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, obs in enumerate(descriptors[\"obs\"]):\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(obs)\n",
    "        title = f\"Frame {i} - {descriptors['task_language_instruction']}\"\n",
    "        for k,v in descriptors['moves'][i].items():\n",
    "            title += f\"\\n------------------------\\n{k} - {v}\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "        display(plt.gcf())\n",
    "        \n",
    "        if wait_for_key:\n",
    "            input(\"Press Enter to continue...\")  # Wait for any key\n",
    "        else:\n",
    "            time.sleep(delay)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_to_episode(traj):\n",
    "    steps = []\n",
    "    for i in range(len(traj[\"observations\"])):\n",
    "        step = {}\n",
    "        step[\"observation\"] = traj[\"observations\"][i]\n",
    "        step[\"action\"] = traj[\"actions\"][i]\n",
    "        steps.append(step)\n",
    "    return dict(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_descriptors(traj):\n",
    "    move_primitives = ecot_primitive_movements.get_move_primitives_episode(repackage_to_episode(traj), threshold=0.00)\n",
    "    move_primitives = [dict(ecot=move) for move in move_primitives]\n",
    "    obs_list = [t[\"images0\"] for t in traj[\"observations\"]]  # list of obs\n",
    "    gt_actions = traj[\"actions\"]  # list of ground truth actions\n",
    "    task_language_instruction = traj[\"language\"][0] if \"language\" in traj else None\n",
    "    return dict(moves=move_primitives, obs=obs_list, gt_actions=gt_actions, task_language_instruction=task_language_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_idx = 1  # Change this to visualize different trajectories\n",
    "episode_descriptors = get_descriptors(bridge_trajs[traj_idx])\n",
    "# show_trajectory_video(episode_descriptors, delay=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, traj in enumerate(bridge_trajs):\n",
    "    descriptors = get_descriptors(traj)\n",
    "    print(i, descriptors['task_language_instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primitive_moves_tester import run_test, print_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = bridge_trajs[traj_idx]\n",
    "episode_descriptors = get_descriptors(bridge_trajs[traj_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def numpy_array_to_png_bytes(arr: np.ndarray) -> bytes:\n",
    "    \"\"\"\n",
    "    Convert a NumPy array to PNG file bytes, as if it was saved as a PNG and then read with fp.read()\n",
    "    \n",
    "    Args:\n",
    "        arr: NumPy array with shape (height, width, 3) and dtype uint8\n",
    "        \n",
    "    Returns:\n",
    "        PNG file bytes\n",
    "    \"\"\"\n",
    "    # Ensure the array is the right shape and type\n",
    "    if len(arr.shape) != 3 or arr.shape[2] != 3:\n",
    "        raise ValueError(f\"Expected array with shape (height, width, 3), got {arr.shape}\")\n",
    "    \n",
    "    if arr.dtype != np.uint8:\n",
    "        arr = arr.astype(np.uint8)\n",
    "    \n",
    "    # Convert the NumPy array to a PIL Image\n",
    "    img = Image.fromarray(arr)\n",
    "    \n",
    "    # Create a BytesIO object to store the image bytes\n",
    "    buffer = io.BytesIO()\n",
    "    \n",
    "    # Save the image to the BytesIO object as PNG\n",
    "    img.save(buffer, format=\"PNG\")\n",
    "    \n",
    "    # Get the bytes from the BytesIO object\n",
    "    png_bytes = buffer.getvalue()\n",
    "    \n",
    "    return png_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_episode_descriptors = dict()\n",
    "sub_step_interval = 2   \n",
    "\n",
    "for k,v in episode_descriptors.items():\n",
    "    if isinstance(v, list):\n",
    "        sub_episode_descriptors[k] = v[::sub_step_interval]\n",
    "    else:\n",
    "        sub_episode_descriptors[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"gpt-4o\"\n",
    "# model = \"gemini/gemini-2.0-flash\"\n",
    "# model = \"gemini/gemini-2.0-pro-exp\"\n",
    "model = \"gemini/gemini-2.5-pro-preview-03-25\"\n",
    "\n",
    "env_desc = \"You are looking at a wooden desk with a black robot arm. The desk has a drawer with a handle and some objects on it.\"\n",
    "task_desc = sub_episode_descriptors[\"task_language_instruction\"]\n",
    "img_bytes = [numpy_array_to_png_bytes(img) for img in sub_episode_descriptors[\"obs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlm_autoeval_robot_benchmark.models.vlm import parse_vlm_responses\n",
    "\n",
    "async def run_episode(model, env_desc, task_desc, img_bytes_list):\n",
    "    vlm = VLM()\n",
    "    reqs = [create_vlm_request(model, img_bytes, env_desc, task_desc) for img_bytes in img_bytes_list]\n",
    "    responses = await vlm.generate_parallel(reqs)\n",
    "    results = parse_vlm_responses(responses)\n",
    "    return results, responses\n",
    "\n",
    "# Run all tests in parallel\n",
    "results, responses = asyncio.run(run_episode(\n",
    "    model,\n",
    "    env_desc,\n",
    "    task_desc,\n",
    "    img_bytes,  # assuming img_bytes is your list of image bytes\n",
    "))\n",
    "print_test_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = [r for r in results if 'answer' not in r]\n",
    "print(f\"num bad: {len(bad)}\")\n",
    "if len(bad) > 0:\n",
    "    print(bad[0]['error'])\n",
    "    print(bad[0]['raw_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, res in enumerate(results):\n",
    "    description = res['description']\n",
    "    sub_episode_descriptors['moves'][t]['vlm - desc'] = \"\\n\".join([description[x:x+100] for x in range(0, len(description), 100)])\n",
    "    action_texts = []\n",
    "    for k in ['x', 'y', 'z', 'tilt', 'roll', 'rotation', 'gripper']:\n",
    "        action_texts.append(f\"{res['answer'][k][0]}\" + (f\"({res['answer'][k][1]})\" if res['answer'][k][1] != '0.0' else \"\"))\n",
    "    sub_episode_descriptors['moves'][t]['vlm'] = \" , \".join([a for a in action_texts if a != 'None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_trajectory_video(sub_episode_descriptors, delay=5, wait_for_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = bridge_trajs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([obs['state'] for obs in traj['observations']])\n",
    "actions = np.array(traj['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.shape, actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(states[i].round(5))\n",
    "print(states[i+1].round(5))\n",
    "print((states[i+1] - states[i]).round(5))\n",
    "\n",
    "print(actions[i].round(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lapa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
