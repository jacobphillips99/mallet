# Rate limit configuration for VLM providers and models
openai:
  gpt-4o:
    requests_per_minute: 5_000
    tokens_per_minute: 800_000
  gpt-4o-mini:
    requests_per_minute: 5_000
    tokens_per_minute: 100_000_000

anthropic:
  claude-3-7-sonnet:
    requests_per_minute: 1_000
    tokens_per_minute: 40_000 # technically input token limit

  claude-3-5-sonnet-20241022:
    requests_per_minute: 1_000
    tokens_per_minute: 80_000 # technically input token limit

  claude-3-5-haiku:
    requests_per_minute: 1_000
    tokens_per_minute: 100_000 # technically input token limit

gemini:
  gemini/gemini-2.0-flash: # weird but makes litellm happy without needing Google Vertex API
    requests_per_minute: 2_000
    tokens_per_minute: 4_000_000 # technically input token limit

  gemini/gemini-2.0-pro-exp:
    requests_per_minute: 20
    tokens_per_minute: 2_000_000 # technically input token limit

  gemini/gemini-2.5-pro-preview-03-25:
    requests_per_minute: 150
    tokens_per_minute: 2_000_000 # technically input token limit
